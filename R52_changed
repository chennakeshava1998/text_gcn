WARNING:tensorflow:From train.py:27: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From train.py:77: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.

WARNING:tensorflow:From train.py:79: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From train.py:81: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/chenna/text_gcn/models.py:143: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/chenna/text_gcn/models.py:41: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/chenna/text_gcn/inits.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/chenna/text_gcn/layers.py:82: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.

WARNING:tensorflow:From /home/chenna/text_gcn/layers.py:26: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.

WARNING:tensorflow:From /home/chenna/text_gcn/layers.py:33: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.

WARNING:tensorflow:From /home/chenna/text_gcn/layers.py:170: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/chenna/text_gcn/models.py:52: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.

WARNING:tensorflow:From /home/chenna/text_gcn/models.py:52: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /home/chenna/text_gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

WARNING:tensorflow:From train.py:91: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From train.py:91: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From train.py:92: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-11-07 06:57:16.317167: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 AVX512F FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-11-07 06:57:16.325245: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000170000 Hz
2019-11-07 06:57:16.325505: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556774301380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-07 06:57:16.325533: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From train.py:105: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

((5879, 300), (5879, 52), (2568, 300), (2568, 52), (15424, 300), (15424, 52))
17992
  (0, 0)	1.0
  (0, 1)	0.8418819135259107
  (0, 2)	0.6863642027209054
  (0, 3)	0.5675274009609849
  (0, 4)	0.6962231574421002
  (0, 5)	0.7747138873760036
  (0, 6)	0.8524074546508552
  (0, 7)	0.8848740942382957
  (0, 8)	0.8759454890695695
  (0, 9)	0.8684921757081706
  (0, 10)	0.789383524174456
  (0, 11)	0.8115222170722107
  (0, 12)	0.8031393494747602
  (0, 13)	0.7673024784622818
  (0, 14)	0.7937993117541188
  (0, 15)	0.9109413283362281
  (0, 16)	0.6214844612456895
  (0, 17)	0.7761511242039351
  (0, 18)	0.830478483583322
  (0, 19)	0.8093056305703467
  (0, 20)	0.7658474956702157
  (0, 21)	0.8650755117320835
  (0, 22)	0.8457785847379428
  (0, 23)	0.6018821035439033
  (0, 24)	0.6741896803015884
  :	:
  (17991, 13757)	6.631123042716941
  (17991, 13840)	2.4827112592245646
  (17991, 13890)	4.336906199393412
  (17991, 14090)	4.941642422609304
  (17991, 14183)	3.8077619951037365
  (17991, 14206)	2.7547272149319464
  (17991, 14207)	5.748733862518468
  (17991, 14262)	4.279747785553464
  (17991, 14288)	5.820192826500612
  (17991, 14337)	18.848449045346175
  (17991, 14410)	4.605170185988092
  (17991, 14425)	6.465414608033325
  (17991, 14454)	4.541318714001559
  (17991, 14547)	5.378360074221573
  (17991, 14600)	14.190740754975378
  (17991, 14611)	2.93394478578831
  (17991, 14617)	2.6545615161512237
  (17991, 14638)	5.402457625800634
  (17991, 14681)	11.715866308966918
  (17991, 14771)	6.631123042716941
  (17991, 14777)	4.881923187907682
  (17991, 14976)	3.874282677445299
  (17991, 15187)	4.471638793363569
  (17991, 15208)	11.429664621685571
  (17991, 15335)	4.167269802126773
(17992, 17992)
(17992, 17992)
17992
Tensor("graphconvolution_2/SparseTensorDenseMatMul/SparseTensorDenseMatMul:0", shape=(?, 52), dtype=float32)
Epoch: 0001 train_loss= 3.95129 train_acc= 0.01820 val_loss= 3.85952 val_acc= 0.41501 time= 18.07662
Epoch: 0002 train_loss= 3.86056 train_acc= 0.43698 val_loss= 3.69562 val_acc= 0.41501 time= 18.19157
Epoch: 0003 train_loss= 3.69939 train_acc= 0.43698 val_loss= 3.44815 val_acc= 0.41501 time= 17.22021
Epoch: 0004 train_loss= 3.45627 train_acc= 0.43698 val_loss= 3.13223 val_acc= 0.41501 time= 17.90240
Epoch: 0005 train_loss= 3.14751 train_acc= 0.43698 val_loss= 2.78147 val_acc= 0.41501 time= 17.83658
Epoch: 0006 train_loss= 2.80778 train_acc= 0.43698 val_loss= 2.44881 val_acc= 0.41501 time= 17.71998
Epoch: 0007 train_loss= 2.47918 train_acc= 0.43698 val_loss= 2.21664 val_acc= 0.41501 time= 17.87320
Epoch: 0008 train_loss= 2.24983 train_acc= 0.43698 val_loss= 2.15623 val_acc= 0.41501 time= 18.66248
Epoch: 0009 train_loss= 2.18832 train_acc= 0.43698 val_loss= 2.23575 val_acc= 0.41501 time= 18.26510
Epoch: 0010 train_loss= 2.26405 train_acc= 0.43698 val_loss= 2.33565 val_acc= 0.41501 time= 16.98007
Epoch: 0011 train_loss= 2.35965 train_acc= 0.43698 val_loss= 2.37942 val_acc= 0.41501 time= 18.43600
Epoch: 0012 train_loss= 2.40298 train_acc= 0.43698 val_loss= 2.35899 val_acc= 0.41501 time= 18.55269
Epoch: 0013 train_loss= 2.37909 train_acc= 0.43698 val_loss= 2.29745 val_acc= 0.41501 time= 17.99979
Epoch: 0014 train_loss= 2.31800 train_acc= 0.43698 val_loss= 2.22687 val_acc= 0.41501 time= 18.67569
Epoch: 0015 train_loss= 2.24892 train_acc= 0.43698 val_loss= 2.17481 val_acc= 0.41501 time= 18.03919
Epoch: 0016 train_loss= 2.19588 train_acc= 0.43698 val_loss= 2.15299 val_acc= 0.41501 time= 18.88430
Epoch: 0017 train_loss= 2.17459 train_acc= 0.43698 val_loss= 2.15070 val_acc= 0.41501 time= 18.27479
Epoch: 0018 train_loss= 2.17084 train_acc= 0.43732 val_loss= 2.14963 val_acc= 0.41654 time= 18.67732
Epoch: 0019 train_loss= 2.16890 train_acc= 0.43936 val_loss= 2.14017 val_acc= 0.42113 time= 19.37385
Epoch: 0020 train_loss= 2.15647 train_acc= 0.44106 val_loss= 2.12171 val_acc= 0.42420 time= 18.83109
Epoch: 0021 train_loss= 2.13485 train_acc= 0.44276 val_loss= 2.09793 val_acc= 0.42420 time= 18.02971
Epoch: 0022 train_loss= 2.10762 train_acc= 0.44446 val_loss= 2.07356 val_acc= 0.42573 time= 18.70072
Epoch: 0023 train_loss= 2.08259 train_acc= 0.44548 val_loss= 2.05334 val_acc= 0.42573 time= 17.38771
Epoch: 0024 train_loss= 2.05763 train_acc= 0.44514 val_loss= 2.03998 val_acc= 0.42573 time= 18.25420
Epoch: 0025 train_loss= 2.04335 train_acc= 0.44548 val_loss= 2.03239 val_acc= 0.42420 time= 17.91431
Epoch: 0026 train_loss= 2.03432 train_acc= 0.44667 val_loss= 2.02631 val_acc= 0.42573 time= 18.39164
Epoch: 0027 train_loss= 2.02756 train_acc= 0.44769 val_loss= 2.01737 val_acc= 0.43492 time= 18.06383
Epoch: 0028 train_loss= 2.02300 train_acc= 0.44940 val_loss= 2.00342 val_acc= 0.44257 time= 18.09895
Epoch: 0029 train_loss= 2.00889 train_acc= 0.45484 val_loss= 1.98499 val_acc= 0.44870 time= 17.66312
Epoch: 0030 train_loss= 1.99573 train_acc= 0.45892 val_loss= 1.96401 val_acc= 0.45636 time= 18.67441
Epoch: 0031 train_loss= 1.97503 train_acc= 0.46419 val_loss= 1.94348 val_acc= 0.46554 time= 19.32493
Epoch: 0032 train_loss= 1.95338 train_acc= 0.46811 val_loss= 1.92626 val_acc= 0.47473 time= 18.30372
Epoch: 0033 train_loss= 1.94056 train_acc= 0.47100 val_loss= 1.91373 val_acc= 0.47473 time= 17.97947
Epoch: 0034 train_loss= 1.92884 train_acc= 0.47032 val_loss= 1.90527 val_acc= 0.47320 time= 17.69853
Epoch: 0035 train_loss= 1.91787 train_acc= 0.46862 val_loss= 1.89690 val_acc= 0.46708 time= 17.93400
Epoch: 0036 train_loss= 1.90933 train_acc= 0.46879 val_loss= 1.88633 val_acc= 0.46708 time= 17.64668
Epoch: 0037 train_loss= 1.89344 train_acc= 0.46981 val_loss= 1.87302 val_acc= 0.47014 time= 18.85126
Epoch: 0038 train_loss= 1.88120 train_acc= 0.47117 val_loss= 1.85799 val_acc= 0.48086 time= 17.84510
Epoch: 0039 train_loss= 1.86490 train_acc= 0.47627 val_loss= 1.84399 val_acc= 0.49464 time= 18.29530
Epoch: 0040 train_loss= 1.85004 train_acc= 0.48903 val_loss= 1.83288 val_acc= 0.50230 time= 14.04817
Epoch: 0041 train_loss= 1.84321 train_acc= 0.49787 val_loss= 1.82267 val_acc= 0.50383 time= 13.76184
Epoch: 0042 train_loss= 1.83463 train_acc= 0.50162 val_loss= 1.81344 val_acc= 0.49770 time= 13.71602
Epoch: 0043 train_loss= 1.82260 train_acc= 0.49447 val_loss= 1.80270 val_acc= 0.49923 time= 13.84126
Epoch: 0044 train_loss= 1.80501 train_acc= 0.49549 val_loss= 1.78967 val_acc= 0.51608 time= 13.72210
Epoch: 0045 train_loss= 1.79414 train_acc= 0.50570 val_loss= 1.77936 val_acc= 0.53905 time= 13.80367
Epoch: 0046 train_loss= 1.78101 train_acc= 0.52254 val_loss= 1.77075 val_acc= 0.54058 time= 13.60640
Epoch: 0047 train_loss= 1.76940 train_acc= 0.52849 val_loss= 1.76126 val_acc= 0.52833 time= 13.48140
Epoch: 0048 train_loss= 1.76215 train_acc= 0.52339 val_loss= 1.75107 val_acc= 0.52527 time= 13.67689
Epoch: 0049 train_loss= 1.74750 train_acc= 0.51760 val_loss= 1.73797 val_acc= 0.53599 time= 13.61854
Epoch: 0050 train_loss= 1.73551 train_acc= 0.52560 val_loss= 1.72452 val_acc= 0.56049 time= 13.44432
Epoch: 0051 train_loss= 1.72912 train_acc= 0.54227 val_loss= 1.71374 val_acc= 0.56355 time= 13.56455
Epoch: 0052 train_loss= 1.71541 train_acc= 0.56013 val_loss= 1.70304 val_acc= 0.56662 time= 13.56998
Epoch: 0053 train_loss= 1.69774 train_acc= 0.55094 val_loss= 1.69276 val_acc= 0.56508 time= 13.59852
Epoch: 0054 train_loss= 1.69252 train_acc= 0.55724 val_loss= 1.68403 val_acc= 0.57427 time= 13.77767
Epoch: 0055 train_loss= 1.67641 train_acc= 0.55758 val_loss= 1.67526 val_acc= 0.57887 time= 13.83180
Epoch: 0056 train_loss= 1.66388 train_acc= 0.57033 val_loss= 1.66391 val_acc= 0.58499 time= 13.66593
Epoch: 0057 train_loss= 1.65468 train_acc= 0.57646 val_loss= 1.65202 val_acc= 0.58652 time= 13.56452
Epoch: 0058 train_loss= 1.64315 train_acc= 0.56948 val_loss= 1.64072 val_acc= 0.58806 time= 13.56447
Epoch: 0059 train_loss= 1.62974 train_acc= 0.58479 val_loss= 1.62915 val_acc= 0.59571 time= 13.55347
Epoch: 0060 train_loss= 1.61740 train_acc= 0.58666 val_loss= 1.61835 val_acc= 0.59571 time= 13.56288
Epoch: 0061 train_loss= 1.60497 train_acc= 0.58683 val_loss= 1.60745 val_acc= 0.60184 time= 13.64856
Epoch: 0062 train_loss= 1.59799 train_acc= 0.59670 val_loss= 1.59511 val_acc= 0.60796 time= 13.51264
Epoch: 0063 train_loss= 1.58205 train_acc= 0.60435 val_loss= 1.58242 val_acc= 0.61103 time= 13.69436
Epoch: 0064 train_loss= 1.57369 train_acc= 0.60673 val_loss= 1.57185 val_acc= 0.60796 time= 13.57821
Epoch: 0065 train_loss= 1.56485 train_acc= 0.60401 val_loss= 1.56037 val_acc= 0.61715 time= 13.53437
Epoch: 0066 train_loss= 1.54790 train_acc= 0.61235 val_loss= 1.55050 val_acc= 0.63553 time= 13.62714
Epoch: 0067 train_loss= 1.53758 train_acc= 0.62987 val_loss= 1.54095 val_acc= 0.62787 time= 13.64444
Epoch: 0068 train_loss= 1.52919 train_acc= 0.62511 val_loss= 1.53076 val_acc= 0.61868 time= 13.75654
Epoch: 0069 train_loss= 1.51179 train_acc= 0.62000 val_loss= 1.51806 val_acc= 0.64165 time= 13.56267
Epoch: 0070 train_loss= 1.49780 train_acc= 0.64024 val_loss= 1.50730 val_acc= 0.64472 time= 13.52045
Epoch: 0071 train_loss= 1.49103 train_acc= 0.64092 val_loss= 1.49673 val_acc= 0.64012 time= 13.69247
Epoch: 0072 train_loss= 1.47628 train_acc= 0.63905 val_loss= 1.48683 val_acc= 0.64319 time= 13.57316
Epoch: 0073 train_loss= 1.46134 train_acc= 0.64314 val_loss= 1.47352 val_acc= 0.65391 time= 13.52566
Epoch: 0074 train_loss= 1.45271 train_acc= 0.64892 val_loss= 1.46136 val_acc= 0.65544 time= 13.63227
Epoch: 0075 train_loss= 1.44430 train_acc= 0.65487 val_loss= 1.45046 val_acc= 0.65237 time= 13.60611
Epoch: 0076 train_loss= 1.43174 train_acc= 0.65300 val_loss= 1.44057 val_acc= 0.65084 time= 13.62621
Epoch: 0077 train_loss= 1.41787 train_acc= 0.65657 val_loss= 1.42804 val_acc= 0.66769 time= 13.51551
Epoch: 0078 train_loss= 1.41491 train_acc= 0.66372 val_loss= 1.41660 val_acc= 0.66922 time= 13.50774
Epoch: 0079 train_loss= 1.39604 train_acc= 0.66661 val_loss= 1.40733 val_acc= 0.66309 time= 13.48178
Epoch: 0080 train_loss= 1.38355 train_acc= 0.66508 val_loss= 1.39757 val_acc= 0.66616 time= 13.60098
Epoch: 0081 train_loss= 1.36960 train_acc= 0.66950 val_loss= 1.38475 val_acc= 0.67381 time= 13.73266
Epoch: 0082 train_loss= 1.36146 train_acc= 0.67307 val_loss= 1.37434 val_acc= 0.67688 time= 13.72806
Epoch: 0083 train_loss= 1.35718 train_acc= 0.66967 val_loss= 1.36676 val_acc= 0.67075 time= 13.93984
Epoch: 0084 train_loss= 1.34235 train_acc= 0.67835 val_loss= 1.35563 val_acc= 0.67228 time= 13.69601
Epoch: 0085 train_loss= 1.32827 train_acc= 0.68158 val_loss= 1.34108 val_acc= 0.68453 time= 13.79991
Epoch: 0086 train_loss= 1.31450 train_acc= 0.68243 val_loss= 1.33472 val_acc= 0.67688 time= 13.65749
Epoch: 0087 train_loss= 1.31357 train_acc= 0.67665 val_loss= 1.32208 val_acc= 0.67841 time= 13.59372
Epoch: 0088 train_loss= 1.29554 train_acc= 0.68532 val_loss= 1.31878 val_acc= 0.67994 time= 13.63079
Epoch: 0089 train_loss= 1.29159 train_acc= 0.68753 val_loss= 1.30130 val_acc= 0.69219 time= 13.56612
Epoch: 0090 train_loss= 1.27423 train_acc= 0.69229 val_loss= 1.29324 val_acc= 0.68300 time= 13.65562
Epoch: 0091 train_loss= 1.26790 train_acc= 0.68566 val_loss= 1.28153 val_acc= 0.68760 time= 13.62869
Epoch: 0092 train_loss= 1.26180 train_acc= 0.68668 val_loss= 1.27774 val_acc= 0.69372 time= 13.66983
Epoch: 0093 train_loss= 1.24514 train_acc= 0.69570 val_loss= 1.27024 val_acc= 0.69219 time= 13.52909
Epoch: 0094 train_loss= 1.23990 train_acc= 0.69859 val_loss= 1.25173 val_acc= 0.69525 time= 13.54477
Epoch: 0095 train_loss= 1.22277 train_acc= 0.70063 val_loss= 1.24732 val_acc= 0.68760 time= 13.32814
Epoch: 0096 train_loss= 1.21765 train_acc= 0.69400 val_loss= 1.23325 val_acc= 0.69985 time= 13.28166
Epoch: 0097 train_loss= 1.20177 train_acc= 0.70335 val_loss= 1.22978 val_acc= 0.70597 time= 13.36525
Epoch: 0098 train_loss= 1.19719 train_acc= 0.70165 val_loss= 1.21813 val_acc= 0.71210 time= 13.28134
Epoch: 0099 train_loss= 1.18801 train_acc= 0.70658 val_loss= 1.20705 val_acc= 0.70444 time= 13.40226
Epoch: 0100 train_loss= 1.17872 train_acc= 0.70335 val_loss= 1.19845 val_acc= 0.70444 time= 13.28011
Epoch: 0101 train_loss= 1.17166 train_acc= 0.70267 val_loss= 1.18980 val_acc= 0.70904 time= 13.28967
Epoch: 0102 train_loss= 1.15546 train_acc= 0.71032 val_loss= 1.18527 val_acc= 0.71210 time= 13.28420
Epoch: 0103 train_loss= 1.15225 train_acc= 0.71152 val_loss= 1.17196 val_acc= 0.70904 time= 13.30713
Epoch: 0104 train_loss= 1.13917 train_acc= 0.71152 val_loss= 1.16522 val_acc= 0.71363 time= 13.39805
Epoch: 0105 train_loss= 1.14012 train_acc= 0.71220 val_loss= 1.15569 val_acc= 0.71363 time= 13.51804
Epoch: 0106 train_loss= 1.13041 train_acc= 0.71271 val_loss= 1.15073 val_acc= 0.72129 time= 13.60053
Epoch: 0107 train_loss= 1.11462 train_acc= 0.71713 val_loss= 1.14388 val_acc= 0.72129 time= 13.36655
Epoch: 0108 train_loss= 1.10833 train_acc= 0.72223 val_loss= 1.13172 val_acc= 0.71669 time= 13.38298
Epoch: 0109 train_loss= 1.10137 train_acc= 0.71968 val_loss= 1.12753 val_acc= 0.71363 time= 13.37210
Epoch: 0110 train_loss= 1.09275 train_acc= 0.71645 val_loss= 1.11627 val_acc= 0.72435 time= 13.34978
Epoch: 0111 train_loss= 1.08614 train_acc= 0.72648 val_loss= 1.11060 val_acc= 0.72894 time= 13.39683
Epoch: 0112 train_loss= 1.07322 train_acc= 0.72478 val_loss= 1.10493 val_acc= 0.72435 time= 13.32262
Epoch: 0113 train_loss= 1.06417 train_acc= 0.72546 val_loss= 1.09500 val_acc= 0.72741 time= 13.39583
Epoch: 0114 train_loss= 1.05668 train_acc= 0.73295 val_loss= 1.08765 val_acc= 0.72741 time= 13.29496
Epoch: 0115 train_loss= 1.04882 train_acc= 0.73499 val_loss= 1.07870 val_acc= 0.72894 time= 13.31731
Epoch: 0116 train_loss= 1.04232 train_acc= 0.73635 val_loss= 1.07260 val_acc= 0.73354 time= 13.35493
Epoch: 0117 train_loss= 1.02839 train_acc= 0.73635 val_loss= 1.06635 val_acc= 0.73660 time= 13.47884
Epoch: 0118 train_loss= 1.02232 train_acc= 0.73992 val_loss= 1.05976 val_acc= 0.73048 time= 13.38491
Epoch: 0119 train_loss= 1.01158 train_acc= 0.73975 val_loss= 1.05341 val_acc= 0.73354 time= 13.33890
Epoch: 0120 train_loss= 1.01389 train_acc= 0.74213 val_loss= 1.04736 val_acc= 0.74119 time= 13.49142
Epoch: 0121 train_loss= 1.00703 train_acc= 0.74588 val_loss= 1.04364 val_acc= 0.73660 time= 13.49070
Epoch: 0122 train_loss= 0.99579 train_acc= 0.74741 val_loss= 1.03234 val_acc= 0.74273 time= 13.38264
Epoch: 0123 train_loss= 0.98884 train_acc= 0.74571 val_loss= 1.02496 val_acc= 0.74273 time= 13.46150
Epoch: 0124 train_loss= 0.98123 train_acc= 0.75098 val_loss= 1.02075 val_acc= 0.74885 time= 13.42474
Epoch: 0125 train_loss= 0.97739 train_acc= 0.75897 val_loss= 1.01295 val_acc= 0.74885 time= 13.48309
Epoch: 0126 train_loss= 0.96783 train_acc= 0.75285 val_loss= 1.01018 val_acc= 0.74579 time= 13.38723
Epoch: 0127 train_loss= 0.95849 train_acc= 0.75336 val_loss= 1.00412 val_acc= 0.75038 time= 13.38857
Epoch: 0128 train_loss= 0.95929 train_acc= 0.75489 val_loss= 0.99877 val_acc= 0.74426 time= 13.45535
Epoch: 0129 train_loss= 0.95413 train_acc= 0.76238 val_loss= 0.99051 val_acc= 0.74579 time= 13.47796
Epoch: 0130 train_loss= 0.93952 train_acc= 0.76578 val_loss= 0.98517 val_acc= 0.75191 time= 13.35946
Epoch: 0131 train_loss= 0.93980 train_acc= 0.75642 val_loss= 0.97671 val_acc= 0.74885 time= 13.31832
Epoch: 0132 train_loss= 0.93205 train_acc= 0.75829 val_loss= 0.97090 val_acc= 0.75191 time= 13.37682
Epoch: 0133 train_loss= 0.92549 train_acc= 0.76306 val_loss= 0.96700 val_acc= 0.75345 time= 13.34646
Epoch: 0134 train_loss= 0.91297 train_acc= 0.77054 val_loss= 0.96192 val_acc= 0.75191 time= 13.38297
Epoch: 0135 train_loss= 0.91144 train_acc= 0.77088 val_loss= 0.95674 val_acc= 0.75804 time= 13.32967
Epoch: 0136 train_loss= 0.90541 train_acc= 0.76918 val_loss= 0.94946 val_acc= 0.76570 time= 13.37614
Epoch: 0137 train_loss= 0.89758 train_acc= 0.77258 val_loss= 0.94440 val_acc= 0.76110 time= 13.31158
Epoch: 0138 train_loss= 0.89428 train_acc= 0.77190 val_loss= 0.94045 val_acc= 0.75957 time= 13.35653
Epoch: 0139 train_loss= 0.88834 train_acc= 0.76731 val_loss= 0.93628 val_acc= 0.76570 time= 13.32374
Epoch: 0140 train_loss= 0.87633 train_acc= 0.77700 val_loss= 0.92870 val_acc= 0.76570 time= 13.42165
Epoch: 0141 train_loss= 0.87485 train_acc= 0.77479 val_loss= 0.92331 val_acc= 0.76417 time= 13.45458
Epoch: 0142 train_loss= 0.86892 train_acc= 0.77921 val_loss= 0.91861 val_acc= 0.76723 time= 13.37974
Epoch: 0143 train_loss= 0.85784 train_acc= 0.77836 val_loss= 0.91707 val_acc= 0.76723 time= 13.40251
Epoch: 0144 train_loss= 0.85939 train_acc= 0.77904 val_loss= 0.91129 val_acc= 0.76876 time= 13.38349
Epoch: 0145 train_loss= 0.85313 train_acc= 0.77956 val_loss= 0.90612 val_acc= 0.77029 time= 13.28315
Epoch: 0146 train_loss= 0.84361 train_acc= 0.78296 val_loss= 0.90486 val_acc= 0.78101 time= 13.29061
Epoch: 0147 train_loss= 0.84470 train_acc= 0.78228 val_loss= 0.89796 val_acc= 0.78407 time= 13.31802
Epoch: 0148 train_loss= 0.84629 train_acc= 0.78024 val_loss= 0.88921 val_acc= 0.78407 time= 13.33264
Epoch: 0149 train_loss= 0.83124 train_acc= 0.78160 val_loss= 0.88767 val_acc= 0.78407 time= 13.45715
Epoch: 0150 train_loss= 0.83072 train_acc= 0.78483 val_loss= 0.88487 val_acc= 0.78254 time= 13.31411
Epoch: 0151 train_loss= 0.81660 train_acc= 0.78857 val_loss= 0.88812 val_acc= 0.77795 time= 13.29581
Epoch: 0152 train_loss= 0.82675 train_acc= 0.78959 val_loss= 0.87303 val_acc= 0.78254 time= 13.47748
Epoch: 0153 train_loss= 0.81052 train_acc= 0.78721 val_loss= 0.87345 val_acc= 0.78407 time= 13.32031
Epoch: 0154 train_loss= 0.81211 train_acc= 0.78415 val_loss= 0.86580 val_acc= 0.78867 time= 13.51995
Epoch: 0155 train_loss= 0.80310 train_acc= 0.80048 val_loss= 0.86933 val_acc= 0.78714 time= 13.48830
Epoch: 0156 train_loss= 0.80339 train_acc= 0.78619 val_loss= 0.85833 val_acc= 0.78714 time= 13.41814
Epoch: 0157 train_loss= 0.80617 train_acc= 0.78381 val_loss= 0.85348 val_acc= 0.78714 time= 13.40739
Epoch: 0158 train_loss= 0.78944 train_acc= 0.79776 val_loss= 0.85312 val_acc= 0.79479 time= 13.38550
Epoch: 0159 train_loss= 0.78693 train_acc= 0.79725 val_loss= 0.84855 val_acc= 0.79326 time= 13.44566
Epoch: 0160 train_loss= 0.78088 train_acc= 0.79759 val_loss= 0.85054 val_acc= 0.79173 time= 13.42569
Epoch: 0161 train_loss= 0.77984 train_acc= 0.79639 val_loss= 0.84152 val_acc= 0.79020 time= 13.37555
Epoch: 0162 train_loss= 0.77595 train_acc= 0.79418 val_loss= 0.83396 val_acc= 0.79479 time= 13.36239
Epoch: 0163 train_loss= 0.76319 train_acc= 0.80014 val_loss= 0.83604 val_acc= 0.78867 time= 13.30517
Epoch: 0164 train_loss= 0.76905 train_acc= 0.80201 val_loss= 0.83072 val_acc= 0.78714 time= 13.61594
Epoch: 0165 train_loss= 0.76189 train_acc= 0.80388 val_loss= 0.83268 val_acc= 0.79020 time= 13.41886
Epoch: 0166 train_loss= 0.74834 train_acc= 0.80099 val_loss= 0.83008 val_acc= 0.79173 time= 13.38555
Epoch: 0167 train_loss= 0.75506 train_acc= 0.79980 val_loss= 0.81593 val_acc= 0.79786 time= 13.39029
Epoch: 0168 train_loss= 0.74304 train_acc= 0.80456 val_loss= 0.81821 val_acc= 0.80398 time= 13.42450
Epoch: 0169 train_loss= 0.74797 train_acc= 0.81255 val_loss= 0.81549 val_acc= 0.79633 time= 13.30751
Epoch: 0170 train_loss= 0.73014 train_acc= 0.81630 val_loss= 0.81712 val_acc= 0.79326 time= 13.33292
Epoch: 0171 train_loss= 0.73424 train_acc= 0.80745 val_loss= 0.81669 val_acc= 0.79173 time= 13.27533
Epoch: 0172 train_loss= 0.73542 train_acc= 0.80150 val_loss= 0.80177 val_acc= 0.80092 time= 13.26929
Epoch: 0173 train_loss= 0.71928 train_acc= 0.80864 val_loss= 0.79883 val_acc= 0.80704 time= 13.33659
Epoch: 0174 train_loss= 0.72558 train_acc= 0.81221 val_loss= 0.79258 val_acc= 0.80245 time= 13.31678
Epoch: 0175 train_loss= 0.72001 train_acc= 0.81426 val_loss= 0.79043 val_acc= 0.80551 time= 13.53579
Epoch: 0176 train_loss= 0.71817 train_acc= 0.80966 val_loss= 0.79307 val_acc= 0.80245 time= 13.40874
Epoch: 0177 train_loss= 0.71344 train_acc= 0.80983 val_loss= 0.80028 val_acc= 0.79479 time= 13.32813
Epoch: 0178 train_loss= 0.71656 train_acc= 0.80949 val_loss= 0.78608 val_acc= 0.80704 time= 13.34431
Epoch: 0179 train_loss= 0.70888 train_acc= 0.81579 val_loss= 0.78605 val_acc= 0.80551 time= 13.35050
Epoch: 0180 train_loss= 0.71173 train_acc= 0.80983 val_loss= 0.77780 val_acc= 0.80858 time= 13.36232
Epoch: 0181 train_loss= 0.69951 train_acc= 0.81477 val_loss= 0.78169 val_acc= 0.80551 time= 13.24744
Epoch: 0182 train_loss= 0.69062 train_acc= 0.81749 val_loss= 0.79038 val_acc= 0.79939 time= 13.29255
Epoch: 0183 train_loss= 0.70611 train_acc= 0.81221 val_loss= 0.77062 val_acc= 0.80704 time= 13.32976
Epoch: 0184 train_loss= 0.68671 train_acc= 0.82174 val_loss= 0.76621 val_acc= 0.81470 time= 13.30689
Epoch: 0185 train_loss= 0.68133 train_acc= 0.82395 val_loss= 0.76612 val_acc= 0.8/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
1623 time= 13.30206
Epoch: 0186 train_loss= 0.69259 train_acc= 0.81970 val_loss= 0.76030 val_acc= 0.81317 time= 13.32696
Epoch: 0187 train_loss= 0.67704 train_acc= 0.82310 val_loss= 0.77374 val_acc= 0.80092 time= 13.58829
Epoch: 0188 train_loss= 0.68000 train_acc= 0.81579 val_loss= 0.76958 val_acc= 0.80704 time= 13.48513
Epoch: 0189 train_loss= 0.67058 train_acc= 0.82242 val_loss= 0.75548 val_acc= 0.81011 time= 13.40164
Epoch: 0190 train_loss= 0.65625 train_acc= 0.82497 val_loss= 0.75167 val_acc= 0.81623 time= 13.44427
Epoch: 0191 train_loss= 0.67300 train_acc= 0.82786 val_loss= 0.74827 val_acc= 0.81623 time= 13.37355
Epoch: 0192 train_loss= 0.66597 train_acc= 0.82701 val_loss= 0.74544 val_acc= 0.81317 time= 13.32615
Epoch: 0193 train_loss= 0.65854 train_acc= 0.82378 val_loss= 0.74850 val_acc= 0.81470 time= 13.35816
Epoch: 0194 train_loss= 0.66217 train_acc= 0.81970 val_loss= 0.74712 val_acc= 0.81776 time= 13.36299
Epoch: 0195 train_loss= 0.65352 train_acc= 0.82752 val_loss= 0.74053 val_acc= 0.82236 time= 13.33927
Epoch: 0196 train_loss= 0.64282 train_acc= 0.83518 val_loss= 0.73789 val_acc= 0.82848 time= 13.27139
Epoch: 0197 train_loss= 0.65079 train_acc= 0.83280 val_loss= 0.73659 val_acc= 0.82389 time= 13.33972
Epoch: 0198 train_loss= 0.63488 train_acc= 0.83518 val_loss= 0.73233 val_acc= 0.82542 time= 13.43946
Epoch: 0199 train_loss= 0.63468 train_acc= 0.83382 val_loss= 0.73220 val_acc= 0.81011 time= 13.28465
Epoch: 0200 train_loss= 0.63951 train_acc= 0.82463 val_loss= 0.73031 val_acc= 0.81776 time= 13.32571
Optimization Finished!
Test set results: cost= 0.68682 accuracy= 0.84891 time= 4.97522
17992
Test Precision, Recall and F1-Score...
              precision    recall  f1-score   support

           0     0.5500    0.8462    0.6667        13
           1     0.4286    0.6000    0.5000         5
           2     0.8333    1.0000    0.9091        20
           3     0.5891    0.8736    0.7037        87
           4     0.0200    0.3333    0.0377         3
           5     0.2188    0.6364    0.3256        11
           6     0.7895    0.6000    0.6818        75
           7     1.0000    0.9333    0.9655        15
           8     0.0000    0.0000    0.0000        12
           9     0.0000    0.0000    0.0000        12
          10     0.0000    0.0000    0.0000         5
          11     0.0000    0.0000    0.0000         1
          12     0.8788    0.8056    0.8406        36
          13     0.8571    0.6667    0.7500         9
          14     0.0000    0.0000    0.0000         2
          15     0.6190    0.6842    0.6500        19
          16     0.0000    0.0000    0.0000         6
          17     0.0000    0.0000    0.0000         1
          18     0.3200    0.6667    0.4324        12
          19     0.0000    0.0000    0.0000         4
          20     0.0000    0.0000    0.0000         2
          21     0.0000    0.0000    0.0000         5
          22     0.3103    0.9000    0.4615        10
          23     0.9524    0.8000    0.8696        25
          24     0.7500    0.6667    0.7059         9
          25     0.0000    0.0000    0.0000         3
          26     1.0000    0.1818    0.3077        11
          27     0.0000    0.0000    0.0000         4
          28     1.0000    0.2469    0.3960        81
          29     0.0000    0.0000    0.0000         4
          30     0.8917    0.8843    0.8880       121
          31     0.9545    0.9545    0.9545        22
          32     0.0000    0.0000    0.0000        12
          33     0.0000    0.0000    0.0000         1
          34     0.0000    0.0000    0.0000         3
          35     0.5714    0.5000    0.5333         8
          36     0.9763    0.9898    0.9830      1083
          37     1.0000    0.6667    0.8000         9
          38     0.6000    0.7500    0.6667         4
          39     0.0000    0.0000    0.0000         9
          40     0.0000    0.0000    0.0000        17
          41     0.1923    1.0000    0.3226        15
          42     0.0000    0.0000    0.0000         1
          43     1.0000    0.6667    0.8000         3
          44     0.0833    0.1111    0.0952         9
          45     0.8125    0.4643    0.5909        28
          46     0.9922    0.9167    0.9529       696
          47     0.1071    0.5000    0.1765         6
          48     0.5000    0.4000    0.4444        10
          49     0.5000    0.1429    0.2222         7
          50     0.0000    0.0000    0.0000         1
          51     0.0000    0.0000    0.0000         1

   micro avg     0.8489    0.8489    0.8489      2568
   macro avg     0.3904    0.3921    0.3584      2568
weighted avg     0.8823    0.8489    0.8508      2568

Macro average Test Precision, Recall and F1-Score...
(0.3903547880080448, 0.3920804842999263, 0.3583501278166359, None)
Micro average Test Precision, Recall and F1-Score...
(0.8489096573208723, 0.8489096573208723, 0.8489096573208724, None)
embeddings:
8892 6532 2568
[[ 0.19196163  0.23366714 -0.04058994 ...  0.04988955  0.14327231
  -0.04420425]
 [ 0.0265368   0.04789519 -0.03403189 ...  0.20143427  0.17130673
  -0.03968648]
 [ 0.9695909   0.9475871  -0.15624887 ...  0.8688577   1.0693532
  -0.167959  ]
 ...
 [ 0.0113819   0.03848726 -0.05208177 ...  0.2505322   0.28691155
  -0.05975093]
 [ 0.05348291  0.10250968 -0.06911321 ...  0.13890164  0.15757841
  -0.07787825]
 [ 0.16482368  0.18589017 -0.04327178 ...  0.3944966   0.35721377
  -0.04945097]]
